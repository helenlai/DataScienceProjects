{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "nQO0Db2fPUnO",
    "outputId": "6f8100d9-1e5a-4595-83fd-0a761ff41a8f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy.stats as stats\n",
    "import keras\n",
    "import bottleneck as bn\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDHTdo-iQYq4"
   },
   "source": [
    "**To reproduce the statistics presented in the result section, please make sure to import all the dataset and weight files in your working directory and define all the functions below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5pKU7KR3pap7"
   },
   "outputs": [],
   "source": [
    "random_seed=0\n",
    "np.random.seed(random_seed)\n",
    "#random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "sns.set(rc={'figure.figsize':(10,8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aaGTS86SlRsX"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input:  \n",
    "mask: known mask (numpy matrix)\n",
    "values: known entries (numpy matrix)\n",
    "shape_mat: shape of the returned matrix (tuple)\n",
    "output: \n",
    "mat: matrix with known values at the known indices and zero elsewhere (numpy matrix)\n",
    "\"\"\"\n",
    "def mask_to_mat(mask,values,shape_mat):\n",
    "    mat=np.zeros_like(shape_mat)\n",
    "    mat[mask]=np.squeeze(np.array(values),axis=0)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4bM6E8PZbgO"
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "input:\n",
    "mat: matrix to binarise\n",
    "output:\n",
    "bin_mat:binarised matrix \n",
    "'''\n",
    "def binarise_mat(mat):\n",
    "    bin_mat=np.zeros(mat.shape)\n",
    "    bin_mat[mat>=4]=1\n",
    "    bin_mat[mat<4]=0\n",
    "    return bin_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szmZ1NNGPUnm"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self,latent_dim,n,random_seed,beta,drop_out_rate,soft_max,correction):\n",
    "        super(VAE,self).__init__()\n",
    "        #Initialising the hyperparameters\n",
    "        self.latent_dim=latent_dim\n",
    "        self.random_seed=random_seed\n",
    "        self.n=n\n",
    "        self.beta=beta\n",
    "        self.drop_out_rate=drop_out_rate\n",
    "        self.soft_max=soft_max\n",
    "        #Initialising dense layers\n",
    "        self.inf_dense_600=tf.keras.layers.Dense(600,activation=tf.nn.relu,kernel_initializer='random_normal')#kernel_initializer=initializer)\n",
    "        self.inf_dense_latent=tf.keras.layers.Dense(2*latent_dim,activation=None,kernel_initializer='random_normal')#kernel_initializer=initializer)\n",
    "        self.gen_dense_600=tf.keras.layers.Dense(600,activation=tf.nn.relu,kernel_initializer='random_normal')#kernel_initializer=initializer)\n",
    "        self.gen_dense_n=tf.keras.layers.Dense(self.n,activation=None,kernel_initializer='random_normal')#kernel_initializer=initializer)\n",
    "        #Initialising dropout layers\n",
    "        self.drop_out=tf.keras.layers.Dropout(rate=self.drop_out_rate,seed=self.random_seed)\n",
    "\n",
    "        self.correction=correction\n",
    "        self.correct_coeff=None\n",
    "        self.miu=None\n",
    "        self.logvar=None\n",
    "        self.z_u=None\n",
    "        self.prob=None\n",
    "        self.logits=None\n",
    "        self.neg_ll=None\n",
    "        self.KL=None\n",
    "        self.neg_ll_to_include=None\n",
    "        \n",
    "    def inference_net(self,input_val):\n",
    "        _,n=input_val.shape\n",
    "    \n",
    "        #dense inference layer [n → 600]\n",
    "        h=self.inf_dense_600(input_val)\n",
    "        if eval==False:\n",
    "            h=self.drop_out(h)\n",
    "        #dense inference layer [600 → 2*latent_dims]\n",
    "        h=self.inf_dense_latent(h)\n",
    "        #selecting the first half of the entries to be the mean of variational distribution\n",
    "        self.miu=h[:,:self.latent_dim]\n",
    "        #selecting the remaining entries to be the log variance of variational distribution\n",
    "        self.logvar=h[:,self.latent_dim:]  \n",
    "        \n",
    "    def reparametrisation_trick(self):\n",
    "        \"\"\"\n",
    "         input:\n",
    "            self.miu:mean of the variational distribution || batch size x latent dimension\n",
    "        output:\n",
    "            self.z_u:sampled latent variable || batch size x latent dimension \n",
    "         \"\"\"\n",
    "        epis=tf.random.normal(tf.shape(self.miu),mean=0,stddev=1,seed=self.random_seed)\n",
    "        self.z_u=epis*tf.exp(0.5*self.logvar)+self.miu\n",
    "    \n",
    "    \n",
    "    def generation_net(self):\n",
    "        #dense generation layer [latent_dims → 600]\n",
    "        h=self.gen_dense_600(self.z_u)\n",
    "        #dense generation layer [600 → n] \n",
    "        if eval==False:\n",
    "            h=self.drop_out(h)\n",
    "        h=self.gen_dense_n(h)\n",
    "        #Saving the unormalised multinomial probability\n",
    "        self.logits=h\n",
    "        if self.soft_max==True:\n",
    "            self.prob=tf.nn.softmax(h)\n",
    "        else:\n",
    "          #applying sigmoid activation to generate probability of an item being one \n",
    "          self.prob=tf.nn.sigmoid(h)\n",
    "        \n",
    "    \n",
    "    def forward_pass(self,input_val):\n",
    "        self.inference_net(input_val)\n",
    "        self.reparametrisation_trick()\n",
    "        self.generation_net()\n",
    "        return self.prob\n",
    "\n",
    "    def prediction(self,input_val):\n",
    "        self.inference_net(input_val)\n",
    "        self.z_u=self.miu\n",
    "        self.generation_net()\n",
    "        return self.prob\n",
    "\n",
    "    def call(self,input_val):\n",
    "        self.forward_pass(input_val)\n",
    "        return self.prob\n",
    "    \n",
    "    \n",
    "    def ELBO(self,input_val,known_mask_batch):\n",
    "        \"\"\" input:\n",
    "                miu:mean of the variational distribution || batch size x latent dimension\n",
    "                logvar:log of the variance of the variational distribution || batch size x latent dimension\n",
    "                log_p: log multinomial probability distribution over items || batch size x n\n",
    "                input_val:input data || batch size x n \n",
    "            output:\n",
    "                ELBO: scalar\n",
    "        \"\"\" \n",
    "        batch_size=self.prob.shape[0]\n",
    "        if self.correction==True:\n",
    "            self.correct_coeff=train_in[known_mask].sum()/known_mask.sum()\n",
    "            self.neg_ll=-((1-self.correct_coeff)*input_val*tf.math.log(1e-7+self.prob)+self.correct_coeff*(1-input_val)*tf.math.log(1e-7+1-self.prob))\n",
    "        else:\n",
    "            self.neg_ll=-(input_val*tf.math.log(1e-7+self.prob)+(1-input_val)*tf.math.log(1e-7+1-self.prob))\n",
    "\n",
    "        self.neg_ll_to_include=tf.reduce_sum(self.neg_ll[known_mask_batch])\n",
    "      \n",
    "        self.neg_11_to_include=(1/batch_size)*self.neg_ll_to_include\n",
    "        \n",
    "        self.KL=tf.reduce_sum(-0.5*(2*self.logvar-tf.exp(self.logvar)-tf.square(self.miu)+1),axis=1)\n",
    "\n",
    "        ELBO=self.neg_ll_to_include+tf.reduce_mean(self.beta*self.KL)\n",
    "        return ELBO\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d5-PAWq0DGgJ"
   },
   "outputs": [],
   "source": [
    "#[1]\n",
    "def recallatN(N,mat_to_eval,predicted,user_level=False): \n",
    "    \"\"\"\n",
    "  input:\n",
    "    N：length of the recommendation list\n",
    "    mat_to_eval:ground truth matrix \n",
    "    predicted:output of the generation network (probability over items)\n",
    "    user_level:if true output the metric score for each user\n",
    "    \n",
    "  output:\n",
    "    self.z_u:sampled latent variable || batch size x latent dimension \n",
    "    \"\"\"\n",
    "    val_size=predicted.shape[0]\n",
    "  #sorting the unormalised multinomial probabilities over items\n",
    "  #such that the first K elements correspond to the largest K elements \n",
    "    sorted_idx=bn.argpartition(-predicted,kth=N)\n",
    "  #selecting the index of the top k elements\n",
    "    top_N_idx=sorted_idx[:,:N]\n",
    "  #creating a boolean matrix of the same shape as the held-out batch\n",
    "    x_pred=np.zeros_like(mat_to_eval,dtype=bool)\n",
    "  #assigning true values for the predicted top k elements \n",
    "    x_pred[np.arange(val_size)[:,np.newaxis],top_N_idx]=True\n",
    "  #creating a bolean array that selects the positive entries in the held-out set\n",
    "    val_positive_bool=mat_to_eval>0\n",
    "  #counting the number of true positive items in the top-k list\n",
    "    correct_positive=np.logical_and(x_pred,val_positive_bool).sum(axis=1).astype(np.float32)\n",
    "  #mean recall for the users in held-out batch\n",
    "    if user_level==True:\n",
    "        to_return=remove_nan(correct_positive/np.minimum(N,mat_to_eval.sum(axis=1)))\n",
    "    else:\n",
    "        to_return=np.nanmean(correct_positive/np.minimum(N,mat_to_eval.sum(axis=1)))\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KHeD4kabv_mh"
   },
   "outputs": [],
   "source": [
    "def binarise(mat):\n",
    "    '''\n",
    "    input:\n",
    "    mat:matrix with probability values to binarise (numpy matrix/array)\n",
    "    output:\n",
    "    bin_mat:binarised matrix with same shape as the input matrix (numpy matrix/array)\n",
    "    '''\n",
    "    bin_mat=np.zeros_like(mat)\n",
    "    bin_mat[mat>=0.5]=1\n",
    "    return bin_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NItnlcFIs_yg"
   },
   "outputs": [],
   "source": [
    "\n",
    "def recon_acc(predicted,mat_to_eval,eval_mask):\n",
    "  '''\n",
    "  input: \n",
    "  predicted:output of the generation network\n",
    "  mat_to_eval:ground truth matrix\n",
    "  eval_mask:boolean mask that select entries for evaluation\n",
    "  output:\n",
    "  reconstruction accurary score for the entries under evaluation\n",
    "  '''\n",
    "  return (binarise(predicted[eval_mask])==mat_to_eval[eval_mask]).sum()/mat_to_eval[eval_mask].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jq8kY3wd3YjZ"
   },
   "outputs": [],
   "source": [
    "  '''\n",
    "  input:\n",
    "  predicted:output of the generation network\n",
    "  eval_entries:ground truth entries\n",
    "  eval_bool: boolean mask that select entries for evaluation\n",
    "  output:\n",
    "  TP:no. of true positives\n",
    "  TN:no. of true negatives\n",
    "  FP:no. of false positives\n",
    "  FN:no. of false negatives\n",
    "  '''\n",
    "def compute_metrics(predicted,eval_entries,eval_bool):\n",
    "    TP=predicted[eval_bool][eval_entries==1].sum()\n",
    "    TN=(predicted[eval_bool][eval_entries==0]==0).sum()\n",
    "    FP=(predicted[eval_bool][eval_entries==0]==1).sum()\n",
    "    FN=(predicted[eval_bool][eval_entries==1]==0).sum()\n",
    "    return TP,TN,FP,FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lG1cgYQFV3TU"
   },
   "outputs": [],
   "source": [
    "def remove_nan(list):\n",
    "    '''\n",
    "    input:\n",
    "    list:list of values \n",
    "    list:list with nan values removed\n",
    "    '''\n",
    "    return np.array(list)[~np.isnan(list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mxKKppMf0Cx1"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function that name each experiment depending on the evaluation mode specified by the boolean parameters\n",
    "'''\n",
    "def name_experiment(hard_gen,soft_max,correction,sampling,pretrained=False):\n",
    "    name=[]\n",
    "    if hard_gen==True:\n",
    "        name.append(\"hard_gen_\")\n",
    "    else:\n",
    "        name.append(\"easy_gen_\")\n",
    "    if soft_max==True:\n",
    "        name.append(\"soft_max_\")\n",
    "    else:\n",
    "        name.append(\"sigmoid_\")\n",
    "    if correction==True:\n",
    "        name.append(\"corrected_\")\n",
    "    else:\n",
    "        name.append(\"nocorrection_\")\n",
    "    if sampling==True:\n",
    "        name.append(\"sampled\")\n",
    "    else:\n",
    "        name.append(\"no_sampling\")\n",
    "    if pretrained==True:\n",
    "        name.append(\"_pretrained\")\n",
    "    if pretrained==False:\n",
    "        name=name[0]+name[1]+name[2]+name[3]\n",
    "    else:\n",
    "        name=name[0]+name[1]+name[2]+name[3]+name[4]\n",
    "    print(\"Evaluation Mode: \"+name)  \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oXt6WSvqkGHq"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function that load train, test, and validation dataset for easy/hard generation task with/without sampling\n",
    "'''\n",
    "def train_loader(hard_gen,sampling):\n",
    "    if hard_gen==False:\n",
    "        if sampling==False:\n",
    "            train_in=sp.load_npz(os.getcwd()+\"/tr_implicit_nosampling_easy.npz\").todense()\n",
    "            known_mask=sp.load_npz(os.getcwd()+\"/known_mask_nosampling_easy.npz\").todense()\n",
    "        else:\n",
    "            train_in=sp.load_npz(os.getcwd()+\"/tr_implicit_sampled_easy.npz\").todense()\n",
    "            known_mask=sp.load_npz(os.getcwd()+\"/known_mask_sampled_easy.npz\").todense()\n",
    "    \n",
    "    else:\n",
    "        if sampling==False:\n",
    "            train_in=sp.load_npz(os.getcwd()+\"/tr_implicit_nosampling_hard.npz\").todense()\n",
    "            known_mask=sp.load_npz(os.getcwd()+\"/known_mask_nosampling_hard.npz\").todense()\n",
    "        else:\n",
    "            train_in=sp.load_npz(os.getcwd()+\"/tr_implicit_sampled_hard.npz\").todense()\n",
    "            known_mask=sp.load_npz(os.getcwd()+\"/known_mask_sampled_hard.npz\").todense()\n",
    "    return train_in,known_mask\n",
    "\n",
    "\n",
    "def test_val_loader(hard_gen,train_in):\n",
    "    if hard_gen==True:\n",
    "        val_fold_in=sp.load_npz(os.getcwd()+\"/val_fold_in_implicit.npz\").todense()\n",
    "        val_report=sp.load_npz(os.getcwd()+\"/val_report_implicit.npz\").todense()\n",
    "        val_fold_in_bool=sp.load_npz(os.getcwd()+\"/val_fold_in_bool.npz\").todense()\n",
    "        val_report_bool=sp.load_npz(os.getcwd()+\"/val_report_bool.npz\").todense()\n",
    "\n",
    "        test_fold_in=sp.load_npz(os.getcwd()+\"/test_fold_in_implicit.npz\").todense()\n",
    "        test_report=sp.load_npz(os.getcwd()+\"/test_report_implicit.npz\").todense()\n",
    "        test_fold_in_bool=sp.load_npz(os.getcwd()+\"/test_fold_in_bool.npz\").todense()\n",
    "        test_report_bool=sp.load_npz(os.getcwd()+\"/test_report_bool.npz\").todense()\n",
    "        return val_fold_in,val_report,val_fold_in_bool,val_report_bool,test_fold_in,test_report,test_fold_in_bool,test_report_bool\n",
    "\n",
    "    else:\n",
    "        val_entries=sp.load_npz(os.getcwd()+\"/val_entries_implicit_easy.npz\").todense()\n",
    "        test_entries=sp.load_npz(os.getcwd()+\"/test_entries_implicit_easy.npz\").todense()\n",
    "        val_bool=sp.load_npz(os.getcwd()+\"/val_bool_easy.npz\").todense()\n",
    "        test_bool=sp.load_npz(os.getcwd()+\"/test_bool_easy.npz\").todense()\n",
    "        val_mat=mask_to_mat(val_bool,val_entries,train_in)\n",
    "        test_mat=mask_to_mat(test_bool,test_entries,train_in)\n",
    "        return val_entries,test_entries,val_bool,test_bool,val_mat,test_mat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TB85wk2U4JhM"
   },
   "outputs": [],
   "source": [
    "def metrics_eval(TP,TN,FN,FP):\n",
    "    '''\n",
    "    input:\n",
    "    TP：no. of true positives\n",
    "    TN: no. of true negatives\n",
    "    FN：no. of false negatives\n",
    "    FP: no. of false positives\n",
    "    output:\n",
    "    recall_val:a single recall score based on all users\n",
    "    neg_recall:a single specificity score based on all users\n",
    "    hit_val: a single precision score based on all users\n",
    "    '''\n",
    "    recall_val=TP/(TP+FN)\n",
    "    neg_recall_val=TN/(TN+FP)\n",
    "    hit_val=TP/(TP+FP)\n",
    "    return recall_val,neg_recall_val,hit_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lVn0B-p4_O--"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function that returns test recall, specificity and precision for VAE-sigmoid and recall@20,recall@50,recall@100 for VAE-softmax\n",
    "'''\n",
    "def load_pred(name,mf=False,hard_gen=False,soft_max=False):\n",
    "\n",
    "    if mf==False:\n",
    "        vae=VAE(parameters.get(\"latent_dim\"),train_in.shape[1],random_seed,parameters.get(\"beta\"),parameters.get(\"drop_out_rate\"),soft_max,correction)\n",
    "        vae.build(train_in.shape)\n",
    "        vae.load_weights(os.getcwd()+'/diff_init2_'+name+\".h5\")\n",
    "        eval=True\n",
    "        if hard_gen==False:\n",
    "            predicted=vae.prediction(train_in)\n",
    "        else:\n",
    "            predicted=vae.prediction(test_fold_in)\n",
    "    else:\n",
    "        predicted=np.array(sp.load_npz(os.getcwd()+\"/predicted_mf.npz\").todense())\n",
    "\n",
    "    if soft_max==False:\n",
    "        if hard_gen==False:\n",
    "            recall_list,neg_recall_list,hit_list=user_level_metrics(binarise(predicted),np.array(test_mat),np.array(test_bool),np.array(test_entries))\n",
    "        else:\n",
    "            recall_list,neg_recall_list,hit_list=user_level_metrics(binarise(predicted),np.array(test_report),np.array(test_report_bool),np.array(test_report[test_report_bool]))\n",
    "        print(\"Test recall:{}\".format(recall_list.mean()))\n",
    "        print(\"Test specificity:{}\".format(neg_recall_list.mean()))\n",
    "        print(\"Test precision:{}\".format(hit_list.mean()))\n",
    "    else:\n",
    "        recall_20_list=recallatN(20,test_report,predicted,user_level=True)\n",
    "        recall_50_list=recallatN(50,test_report,predicted,user_level=True)\n",
    "        recall_100_list=recallatN(100,test_report,predicted,user_level=True)\n",
    "        print(\"recall@20:{}\".format(recallatN(20,test_report,predicted)))\n",
    "        print(\"recall@50:{}\".format(recallatN(50,test_report,predicted)))\n",
    "        print(\"recall@100:{}\".format(recallatN(100,test_report,predicted)))\n",
    "\n",
    "    if soft_max==False:\n",
    "        if mf==False:\n",
    "            return recall_list,neg_recall_list,hit_list,vae\n",
    "        else:\n",
    "            return recall_list,neg_recall_list,hit_list\n",
    "    else:\n",
    "        return recall_20_list,recall_50_list,recall_100_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v8uw7dJGMIS0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function that loads TN,TP,FN,FP given an experiment name for VAE-sigmoid and recall@20 for VAE-softmax\n",
    "'''\n",
    "def load_metrics(name,soft_max=False):\n",
    "    if soft_max==False:\n",
    "        TN=np.load(os.getcwd()+\"/TN_list_\"+name+\".npy\")\n",
    "        TP=np.load(os.getcwd()+\"/TP_list_\"+name+\".npy\")\n",
    "        FN=np.load(os.getcwd()+\"/FN_list_\"+name+\".npy\")\n",
    "        FP=np.load(os.getcwd()+\"/FP_list_\"+name+\".npy\")\n",
    "        return TN,TP,FN,FP\n",
    "    else:\n",
    "        recallat20=np.load(os.getcwd()+\"/recallat20\"+name+\".npy\")\n",
    "        return recallat20\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "17UMyhX4_1gL"
   },
   "outputs": [],
   "source": [
    "def user_level_metrics(predicted,mat,eval_bool,eval_entries):\n",
    "    '''\n",
    "    input:\n",
    "    predicted:output of the generation network\n",
    "    mat:ground truth matrix\n",
    "    eval_bool:boolean mask that return the values for evaluation\n",
    "    eval_entries: ground truth values \n",
    "    output:\n",
    "    list of per user recall value with nan values removed\n",
    "    list of per user specificity value with nan values removed\n",
    "    list of per user precision value with nan values removed\n",
    "    '''\n",
    "    recall_list=[]\n",
    "    hit_list=[]\n",
    "    neg_recall_list=[]\n",
    "\n",
    "    for i in range(predicted.shape[0]):\n",
    "        recall_list.append(predicted[i][mat[i]==1].sum()/(mat[i]==1).sum())\n",
    "        #recon_acc_list.append((predicted[i][eval_bool[i]]==mat[i][eval_bool[i]]).sum()/mat[i][eval_bool[i]].shape[0])\n",
    "        neg_recall_list.append((((predicted[i][eval_bool[i]])[mat[i][eval_bool[i]]==0])==0).sum()/(mat[i][eval_bool[i]]==0).sum())\n",
    "        hit_list.append(np.logical_and(predicted[i][eval_bool[i]]==1,mat[i][eval_bool[i]]==1,mat[i][eval_bool[i]]==1).sum()/((predicted[i][eval_bool[i]])==1).sum())\n",
    "    return remove_nan(recall_list),remove_nan(neg_recall_list),remove_nan(hit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZVNxQ4GrGDuG"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function that performs paired sample t-test for user-level recall,specificity for VAE-sigmoid, and user-level recall@20,50,100 for VAE-softmax\n",
    "independent t-test is performed for precision \n",
    "'''\n",
    "def comparison(list11,list12,list21,list22,list31,list32,soft_max=False):\n",
    "    if soft_max==False:\n",
    "        print(\"recall: {}\".format(stats.ttest_rel(list11,list12)))\n",
    "        print(\"specificity: {}\".format(stats.ttest_rel(list21,list22)))\n",
    "        print(\"precision: {}\".format(stats.ttest_ind(list31,list32)))\n",
    "\n",
    "    else:\n",
    "        print(\"recall@20: {}\".format(stats.ttest_rel(list11,list12)))\n",
    "        print(\"recall@50: {}\".format(stats.ttest_rel(list21,list22)))\n",
    "        print(\"recall@100: {}\".format(stats.ttest_rel(list31,list32)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEYEgr2JYrj7"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function that names and load the train,validation and test set for a given evaluation mode\n",
    "'''\n",
    "def name_load(hard_gen,soft_max,sampling,correction,pretrained=False):\n",
    "    name=name_experiment(hard_gen,soft_max,correction,sampling,pretrained)\n",
    "    train_in,known_mask=train_loader(hard_gen,sampling)\n",
    "    if hard_gen==False:\n",
    "        val_entries,test_entries,val_bool,test_bool,val_mat,test_mat=test_val_loader(hard_gen,train_in)\n",
    "        assert train_in[test_bool].sum()==0\n",
    "        assert train_in[val_bool].sum()==0\n",
    "        return name,train_in,known_mask,val_entries,test_entries,val_bool,test_bool,val_mat,test_mat\n",
    "    else:\n",
    "        val_fold_in,val_report,val_fold_in_bool,val_report_bool,test_fold_in,test_report,test_fold_in_bool,test_report_bool=test_val_loader(hard_gen,train_in)\n",
    "        return name,train_in,known_mask,val_fold_in,val_report,val_fold_in_bool,val_report_bool,test_fold_in,test_report,test_fold_in_bool,test_report_bool\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opupw6jRcYay"
   },
   "outputs": [],
   "source": [
    "parameters={'beta': 0.779,\n",
    " 'drop_out_rate': 0.132,\n",
    " 'latent_dim': 15,\n",
    " 'lr': 0.00013}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9L0HQ1ZsyX4"
   },
   "source": [
    "**VAE-sigmoid without correction vs. MF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "49PK4MvexUEn",
    "outputId": "ff8c0d51-8431-4ea7-a70c-6daa44536f38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Mode: easy_gen_sigmoid_nocorrection_no_sampling\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hard_gen=False\n",
    "soft_max=False\n",
    "sampling=False\n",
    "correction=False\n",
    "name1,train_in,known_mask,val_entries,test_entries,val_bool,test_bool,val_mat,test_mat=name_load(hard_gen,soft_max,sampling,correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 821
    },
    "colab_type": "code",
    "id": "-KgCyoqnJLM0",
    "outputId": "4b6253da-c4d7-4c44-9a57-2047eeda20f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_12 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in long_scalars\n",
      "C:\\Users\\helen\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in long_scalars\n",
      "C:\\Users\\helen\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test recall:0.6386643481443509\n",
      "Test specificity:0.5016177545890661\n",
      "Test precision:0.7464510520590322\n",
      "easy_gen_sigmoid_nocorrection_no_sampling\n"
     ]
    }
   ],
   "source": [
    "recall_list_1,neg_recall_list_1,hit_list_1,vae_1=load_pred(name1)\n",
    "print(name1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in long_scalars\n",
      "C:\\Users\\helen\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in long_scalars\n",
      "C:\\Users\\helen\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test recall:0.537495489584306\n",
      "Test specificity:0.47231367600220064\n",
      "Test precision:0.6969877583436009\n"
     ]
    }
   ],
   "source": [
    "recall_list_mf,neg_recall_list_mf,hit_list_mf=load_pred(\"mf\",mf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "hzLGOz5Sbx3O",
    "outputId": "397d161a-31a3-4552-d53c-7713448eba4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: Ttest_relResult(statistic=8.843816266531107, pvalue=1.5331809141008722e-18)\n",
      "specificity: Ttest_relResult(statistic=1.8060603649456421, pvalue=0.07109854077880035)\n",
      "precision: Ttest_indResult(statistic=4.328196088193042, pvalue=1.531838868096496e-05)\n"
     ]
    }
   ],
   "source": [
    "comparison(recall_list_1,recall_list_mf,neg_recall_list_1,neg_recall_list_mf,hit_list_1,hit_list_mf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R1r0vCDcs-tW"
   },
   "source": [
    "**VAE-sigmoid with correction vs. MF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "oatxO1wyxbuJ",
    "outputId": "76a12af5-cf60-4cc6-d37d-facc6355c785"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Mode: easy_gen_sigmoid_corrected_no_sampling\n"
     ]
    }
   ],
   "source": [
    "correction=True\n",
    "hard_gen=False\n",
    "soft_max=False\n",
    "sampling=False\n",
    "name2,train_in,known_mask,val_entries,test_entries,val_bool,test_bool,val_mat,test_mat=name_load(hard_gen,soft_max,sampling,correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_76 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in long_scalars\n",
      "C:\\Users\\helen\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\helen\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test recall:0.7139755674441481\n",
      "Test specificity:0.5107054148037755\n",
      "Test precision:0.7688982488626313\n",
      "easy_gen_sigmoid_corrected_no_sampling\n"
     ]
    }
   ],
   "source": [
    "recall_list_2,neg_recall_list_2,hit_list_2,vae_2=load_pred(name2)\n",
    "print(name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "vmauyoX5NSGB",
    "outputId": "9191db28-c077-4209-d038-c640d7639f33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test recall:0.537495489584306\n",
      "Test specificity:0.47231367600220064\n",
      "Test precision:0.6969877583436009\n"
     ]
    }
   ],
   "source": [
    "recall_list_mf,neg_recall_list_mf,hit_list_mf=load_pred(\"mf\",mf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "iBbMplEhmhYe",
    "outputId": "0c45764e-23f4-4f76-b55b-fd5db0858bf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: Ttest_relResult(statistic=15.837626451538037, pvalue=2.361501259660995e-54)\n",
      "specificity: Ttest_relResult(statistic=2.35870777479238, pvalue=0.0184594798292878)\n",
      "precision: Ttest_indResult(statistic=6.5037987691068055, pvalue=8.543855101749583e-11)\n"
     ]
    }
   ],
   "source": [
    "comparison(recall_list_2,recall_list_mf,neg_recall_list_2,neg_recall_list_mf,hit_list_2,hit_list_mf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhtMUSButKd4"
   },
   "source": [
    "**Effect of sampling: VAE-sigmoid easy generalisation task with correction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "colab_type": "code",
    "id": "vpXqcnTGxjLt",
    "outputId": "2724a1ed-5691-48f3-a3da-a5ae7148a1e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Mode: easy_gen_sigmoid_corrected_sampled\n",
      "WARNING:tensorflow:Layer dense_84 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test recall:0.6640636285197042\n",
      "Test specificity:0.5713222948878687\n",
      "Test precision:0.7797279441562427\n",
      "easy_gen_sigmoid_corrected_sampled\n"
     ]
    }
   ],
   "source": [
    "correction=True\n",
    "sampling=True\n",
    "hard_gen=False\n",
    "soft_max=False\n",
    "name4,train_in,known_mask,val_entries,test_entries,val_bool,test_bool,val_mat,test_mat=name_load(hard_gen,soft_max,sampling,correction)\n",
    "recall_list_4,neg_recall_list_4,hit_list_4,vae_4=load_pred(name4)\n",
    "print(name4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "colab_type": "code",
    "id": "7dT4kzv0xmqf",
    "outputId": "16dd70ef-3bfd-47ce-e12e-fdf015a6709f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Mode: easy_gen_sigmoid_corrected_no_sampling\n",
      "WARNING:tensorflow:Layer dense_32 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in long_scalars\n",
      "C:\\Users\\helen\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\helen\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test recall:0.7139755674441481\n",
      "Test specificity:0.5107054148037755\n",
      "Test precision:0.7688982488626313\n",
      "easy_gen_sigmoid_corrected_no_sampling\n"
     ]
    }
   ],
   "source": [
    "hard_gen=False\n",
    "sampling=False\n",
    "correction=True\n",
    "soft_max=False\n",
    "name5,train_in,known_mask,val_entries,test_entries,val_bool,test_bool,val_mat,test_mat=name_load(hard_gen,soft_max,sampling,correction)\n",
    "recall_list_5,neg_recall_list_5,hit_list_5,vae_5=load_pred(name5)\n",
    "print(name5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "fE7P7PlfmhkM",
    "outputId": "1ca4af6b-31d4-43fe-814b-e5c0fdb35431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: Ttest_relResult(statistic=-7.45035846227843, pvalue=1.2052303059460574e-13)\n",
      "specificity: Ttest_relResult(statistic=6.173744041759997, pvalue=8.455161201998364e-10)\n",
      "precision: Ttest_indResult(statistic=1.0522782947446474, pvalue=0.29271786442978087)\n"
     ]
    }
   ],
   "source": [
    "comparison(recall_list_4,recall_list_5,neg_recall_list_4,neg_recall_list_5,hit_list_4,hit_list_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ukutViLhuFBx"
   },
   "source": [
    "**Effect of Sampling: VAE-sigmoid hard generalisation task with correction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "colab_type": "code",
    "id": "WeErRmcROi-f",
    "outputId": "3ff407e2-b998-4a88-f930-650fa75ef612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Mode: hard_gen_sigmoid_corrected_no_sampling\n",
      "WARNING:tensorflow:Layer dense_92 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Test recall:0.814877426507503\n",
      "Test specificity:0.5762532051004097\n",
      "Test precision:0.7898134076325567\n",
      "hard_gen_sigmoid_corrected_no_sampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "correction=True\n",
    "sampling=False\n",
    "hard_gen=True\n",
    "soft_max=False\n",
    "name6,train_in,known_mask,val_fold_in,val_report,val_fold_in_bool,val_report_bool,test_fold_in,test_report,test_fold_in_bool,test_report_bool=name_load(hard_gen,soft_max,sampling,correction)\n",
    "recall_list_6,neg_recall_list_6,hit_list_6,vae_6=load_pred(name6,hard_gen=True)\n",
    "print(name6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "colab_type": "code",
    "id": "aU1NJAXKOp53",
    "outputId": "413507e5-1799-4d04-e87d-09e6f9fda6b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Mode: hard_gen_sigmoid_corrected_sampled\n",
      "WARNING:tensorflow:Layer dense_96 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Test recall:0.751631389975218\n",
      "Test specificity:0.64570726639469\n",
      "Test precision:0.8202575867973243\n",
      "hard_gen_sigmoid_corrected_sampled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "correction=True\n",
    "sampling=True\n",
    "hard_gen=True\n",
    "soft_max=False\n",
    "name7,train_in,known_mask,val_fold_in,val_report,val_fold_in_bool,val_report_bool,test_fold_in,test_report,test_fold_in_bool,test_report_bool=name_load(hard_gen,soft_max,sampling,correction)\n",
    "recall_list_7,neg_recall_list_7,hit_list_7,vae_7=load_pred(name7,hard_gen=True)\n",
    "print(name7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "mnfgFyy-og-S",
    "outputId": "ee0c554b-e53a-49c4-a653-19c1232c6fcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: Ttest_relResult(statistic=3.4946226866049455, pvalue=0.0005332777994444515)\n",
      "specificity: Ttest_relResult(statistic=-2.169558978583597, pvalue=0.03103460495050768)\n",
      "precision: Ttest_indResult(statistic=-1.2493113039559953, pvalue=0.211955536087552)\n"
     ]
    }
   ],
   "source": [
    "comparison(recall_list_6,recall_list_7,neg_recall_list_6,neg_recall_list_7,hit_list_6,hit_list_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XqiYD5TrvJdM"
   },
   "source": [
    "**Effect of Sampling: VAE-softmax hard generalisation task without correction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "H8By6wYQO1EJ",
    "outputId": "d0361a76-fb01-44c7-d8a0-bb7c579bf587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense_96 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@20:0.3441278684465418\n",
      "recall@50:0.4393811356819726\n",
      "recall@100:0.5232486234353879\n",
      "hard_gen_soft_max_nocorrection_no_sampling\n"
     ]
    }
   ],
   "source": [
    "correction=False\n",
    "hard_gen=True\n",
    "sampling=False\n",
    "soft_max=True\n",
    "name8,train_in,known_mask,val_fold_in,val_report,val_fold_in_bool,val_report_bool,test_fold_in,test_report,test_fold_in_bool,test_report_bool=name_load(hard_gen,soft_max,sampling,correction)\n",
    "recall_20_list_8,recall_50_list_8,recall_100_list_8=load_pred(name8,mf=False,hard_gen=True,soft_max=True)\n",
    "print(name8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "ic-fFX6FO1G4",
    "outputId": "cc26374c-6a66-4b0e-9962-6e599ee66f46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard_gen_soft_max_nocorrection_sampled\n",
      "WARNING:tensorflow:Layer dense_92 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@20:0.3168542464504973\n",
      "recall@50:0.4163602632977888\n",
      "recall@100:0.48468431251425426\n"
     ]
    }
   ],
   "source": [
    "correction=False\n",
    "hard_gen=True\n",
    "sampling=True\n",
    "soft_max=True\n",
    "name9,train_in,known_mask,val_fold_in,val_report,val_fold_in_bool,val_report_bool,test_fold_in,test_report,test_fold_in_bool,test_report_bool=name_load(hard_gen,soft_max,sampling,correction)\n",
    "recall_20_list_9,recall_50_list_9,recall_100_list_9=load_pred(name9,mf=False,hard_gen=True,soft_max=True)\n",
    "print(name9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "X1UCrC5pohBk",
    "outputId": "69d5dcda-2450-45b0-b0fe-a141835a1640"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@20: Ttest_relResult(statistic=1.6357543451015173, pvalue=0.10275536798998279)\n",
      "recall@50: Ttest_relResult(statistic=1.341304502956455, pvalue=0.18065788423343307)\n",
      "recall@100: Ttest_relResult(statistic=2.1449538998387307, pvalue=0.0326178800654658)\n"
     ]
    }
   ],
   "source": [
    "comparison(recall_20_list_8,recall_20_list_9,recall_50_list_8,recall_50_list_9,recall_100_list_8,recall_100_list_9,soft_max=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xogmmrFqvK8h"
   },
   "source": [
    "**Effect of side information: VAE-sigmoid easy generalisation task without correction or sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "colab_type": "code",
    "id": "H9keZ4fnmeZx",
    "outputId": "0cf9d5dc-daab-4273-e1dd-cf8d35a91c23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Mode: easy_gen_sigmoid_nocorrection_no_sampling_pretrained\n",
      "WARNING:tensorflow:Layer dense_116 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test recall:0.6511300241573013\n",
      "Test specificity:0.5189819098630574\n",
      "Test precision:0.7605831116869459\n",
      "easy_gen_sigmoid_nocorrection_no_sampling_pretrained\n"
     ]
    }
   ],
   "source": [
    "correction=False\n",
    "sampling=False\n",
    "hard_gen=False\n",
    "soft_max=False\n",
    "name11,train_in,known_mask,val_entries,test_entries,val_bool,test_bool,val_mat,test_mat=name_load(hard_gen,soft_max,sampling,correction,pretrained=True)\n",
    "recall_list_11,neg_recall_list_11,hit_list_11,vae_11=load_pred(name11,hard_gen=False)\n",
    "print(name11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Mode: easy_gen_sigmoid_corrected_no_sampling\n",
      "WARNING:tensorflow:Layer dense_56 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in long_scalars\n",
      "C:\\Users\\helen\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\helen\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test recall:0.7139755674441481\n",
      "Test specificity:0.5107054148037755\n",
      "Test precision:0.7688982488626313\n",
      "easy_gen_sigmoid_corrected_no_sampling\n"
     ]
    }
   ],
   "source": [
    "hard_gen=False\n",
    "sampling=False\n",
    "correction=True\n",
    "soft_max=False\n",
    "name5,train_in,known_mask,val_entries,test_entries,val_bool,test_bool,val_mat,test_mat=name_load(hard_gen,soft_max,sampling,correction)\n",
    "recall_list_5,neg_recall_list_5,hit_list_5,vae_5=load_pred(name5)\n",
    "print(name5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "Igxc_gVv2dfS",
    "outputId": "9bd2fc7b-11e5-448a-ab95-da50c0c87073"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: Ttest_relResult(statistic=6.893051124676405, pvalue=6.6080197054983125e-12)\n",
      "specificity: Ttest_relResult(statistic=-0.6331077412274727, pvalue=0.5267546200058721)\n",
      "precision: Ttest_indResult(statistic=0.798608537235049, pvalue=0.42455199363506824)\n"
     ]
    }
   ],
   "source": [
    "comparison(recall_list_5,recall_list_11,neg_recall_list_5,neg_recall_list_11,hit_list_5,hit_list_11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHP3eqTNvLz2"
   },
   "source": [
    "**Effect of side information: VAE-softmax hard generalisation task without correction or sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "colab_type": "code",
    "id": "gn7x2Aqbmo4c",
    "outputId": "259fdd5b-bf9e-4cb7-cc20-9c78054133d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Mode: hard_gen_soft_max_nocorrection_no_sampling_pretrained\n",
      "WARNING:tensorflow:Layer dense_120 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@20:0.298105838725882\n",
      "recall@50:0.3800729357960792\n",
      "recall@100:0.45124982438062183\n",
      "hard_gen_soft_max_nocorrection_no_sampling_pretrained\n"
     ]
    }
   ],
   "source": [
    "correction=False\n",
    "hard_gen=True\n",
    "sampling=False\n",
    "soft_max=True\n",
    "name12,train_in,known_mask,val_fold_in,val_report,val_fold_in_bool,val_report_bool,test_fold_in,test_report,test_fold_in_bool,test_report_bool=name_load(hard_gen,soft_max,sampling,correction,pretrained=True)\n",
    "recall_20_list_12,recall_50_list_12,recall_100_list_12=load_pred(name12,mf=False,hard_gen=True,soft_max=True)\n",
    "print(name12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "colab_type": "code",
    "id": "pEeWkPzfm8zQ",
    "outputId": "17e2611e-6d19-4a15-fdf8-537ca9b51b10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Mode: hard_gen_soft_max_nocorrection_no_sampling\n",
      "WARNING:tensorflow:Layer dense_124 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in true_divide\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@20:0.3441278684465418\n",
      "recall@50:0.4393811356819726\n",
      "recall@100:0.5232486234353879\n",
      "hard_gen_soft_max_nocorrection_no_sampling\n"
     ]
    }
   ],
   "source": [
    "correction=False\n",
    "hard_gen=True\n",
    "sampling=False\n",
    "soft_max=True\n",
    "name8,train_in,known_mask,val_fold_in,val_report,val_fold_in_bool,val_report_bool,test_fold_in,test_report,test_fold_in_bool,test_report_bool=name_load(hard_gen,soft_max,sampling,correction)\n",
    "recall_20_list_8,recall_50_list_8,recall_100_list_8=load_pred(name8,mf=False,hard_gen=True,soft_max=True)\n",
    "print(name8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "gA30hGG9o9El",
    "outputId": "6d322de8-7a4d-4f51-9ac1-93589e8ee2f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@20: Ttest_relResult(statistic=2.4217609348996345, pvalue=0.015933459174520742)\n",
      "recall@50: Ttest_relResult(statistic=3.295272518581719, pvalue=0.0010798728050247972)\n",
      "recall@100: Ttest_relResult(statistic=4.135382338798507, pvalue=4.403645551300788e-05)\n"
     ]
    }
   ],
   "source": [
    "comparison(recall_20_list_8,recall_20_list_12,recall_50_list_8,recall_50_list_12,recall_100_list_8,recall_100_list_12,soft_max=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] Dawen Liang. dawenl/vae_cf. Retrieved September 11, 2020 from https://github.com/dawenl/vae_cf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Reproducible Script.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
